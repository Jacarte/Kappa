

\msection{Software diversification}
\label{sota:sw}

%%ORIGINAL
Software diversification involves the synthesis, reuse, distribution, and execution of different, functionally equivalent programs. 
As outlined in Baudry \etal's survey \cite{natural_diversity}, software diversification falls into five usage categories: reusability \cite{pohl2005software},performance \cite{10.1145/2025113.2025133}, fault tolerance \cite{1659219}, software testing \cite{Chen2010AdaptiveRT}, and security \cite{cohen1993operating}. 
Our work specifically contributes to the last two categories.
Based on the works of Cohen \etal \cite{cohen1993operating}, Forrest \etal \cite{595185}, Jackson \etal \cite{jackson} and Baudry \etal \cite{natural_diversity}, this section presents core concepts and related works, emphasizing how they generate diversification and apply it to \Wasm.

\msubsection{Generation of Software Variants}
\label{generation}

Software variants are functionally equivalent versions of an original program, created through software diversification at different stages of the software lifecycle, such as the source code or machine code levels. 
%\todo{Two sentences about when: Design and implementation, Compilation and linking, post Installation, and load time.}
The diversification can be either natural \cite{natural_diversity} or artificial \cite{offensive_div}.

\begin{strategy}[Natural Diversity]
    \label{natural_diversity}
    Natural diversity denotes the innate process wherein humans create software variants using various programming languages, compilers, and operating systems \cite{natural_diversity}, all adhering to the same initial functional requirements.
    For instance, Firefox and Chrome web browsers exemplify natural diversity, given their practical differences, they serve the same purpose.
    Natural diversity plays a crucial role in securing systems, as different variants are not susceptible to identical vulnerabilities.
    Creating natural software variants demands a significant amount of human effort and time. 
    This makes it impractical for new ecosystems such as \Wasm.
    On the other hand, humans are not always the best at creating diverse software variants since they think alike \cite{feud}.
    Thus, systematic and artificial diversification approaches are better.
\end{strategy}


\begin{strategy}[Artificial Diversity]
    \label{artificial_diversity}
    The concept of artificial software variants starts with Randell's 1975 work \cite{10.1145/390016.808467}, which put forth the notion of artificial fault-tolerant instruction blocks. 
    Artificial software diversification, as proposed by Cohen and Forrest in the 1990s \cite{cohen1993operating, 595185}, gets its development through rewriting strategies. 
    These strategies consist of rule sets for modifying software components to create functionally equivalent, yet distinct, programs. 
    Rewriting strategies typically take the form of tuples: \texttt{instr1 => (instr2, instr3, ...)}, where \texttt{instr} represents the original code and \texttt{(instr2, instr3, ...)} denotes the functionally equivalent code.
    This dissertation focuses on artificial software diversification, as it is more practical, systematic, and scalable than natural diversity.
\end{strategy}

\begin{strategy}[Rewriting strategy]
    \label{rewriting_strategy}
    The creation of artificial software diversification commences with rewriting rules.
    A rewriting rule refers to a functionally equivalent substitution for a code segment, manually written. 
    These rules can be applied at varying levels, from coarse to fine-grained. 
    This can range from the program dependencies level \cite{Harrand1650630} to the instruction level \cite{offensive_div}. 
    For example, Cleemput et al.~\cite{Cleemput2012} and Homescu et al.~\cite{homescu2013profile} inject NOP instructions to yield statically varied versions at the instruction level. 
    Here, the rewriting rule is represented as \texttt{instr => (nop instr)}, signifying a \texttt{nop} operation preceding the instruction.


    %Although the studies by Cleemput et al. and Homescu et al. are easily applicable to \Wasm, this specific strategy may fall under the \emph{non preserved} category since \Wasm typically compiles later. 
    %This implies that JIT compilers could nullify this diversification strategy by merely applying straightforward optimizations.
\end{strategy}



\begin{strategy}[Instruction Reordering]
    \label{instruction_reordering}
    This strategy reorders instructions in a program.
    For example, variable declarations may change if compilers reorder them in the symbol tables. 
    This prevents static examination and analysis of parameters and alters memory locations. 
    In this area, Bhatkar \etal \cite{bhatkar03, bhatkar2005efficient} proposed the random permutation of variable and routine order for ELF binaries.
    Such strategies are not implemented for \Wasm to the best of our knowledge.
\end{strategy}


\begin{strategy}[Adding, Changing, Removing Jumps and Calls]
    \label{jumps}
    This strategy generates program variants by adding, changing, or removing jumps and calls in the original program. 
    Cohen \cite{cohen1993operating} primarily illustrated this concept by inserting random jumps in programs. Pettis and Hansen \cite{pettisochhansen} suggested splitting basic blocks and functions for the PA-RISC architecture, inserting jumps between splits.
    Similarly, Crane \etal~\cite{crane2015thwarting} de-inlined basic blocks of code as an LLVM pass. 
    In their approach, each de-inlined code transforms into semantically equivalent functions that are randomly selected at runtime to replace the original code calculation. 
    On the same topic, Bhatkar \etal \cite{bhatkar2005efficient} extended their previous approach \cite{bhatkar03}, replacing function calls with indirect pointer calls in C source code, allowing post-binary reordering of function calls. 
    In the \Wasm context, the most analogous work is wobfuscator \cite{wobfuscator}.
    Wobfuscator, a JavaScript obfuscator, substitutes JavaScript code with \Wasm code, e.g., numeric calcula.
    This strategy effectively uses the interleaving of calls between JavaScript and \Wasm to provide JavaScript variants.
\end{strategy}


\begin{strategy}[Program Memory and Stack Randomization]
    \label{mem_strategy}
    This strategy alters the layout of programs in the host memory. 
    Additionally, it can randomize how a program variant operates its memory. 
    The work of Bhatkar \etal \cite{bhatkar03, bhatkar2005efficient} proposes to randomize the base addresses of applications and library memory regions in ELF binaries. 
    Tadesse Aga and Autin \cite{aga2019smokestack}, and Lee \etal \cite{lee2021savior} propose a technique to randomize the local stack organization for function calls using a custom LLVM compiler.
    Younan \etal \cite{Younan2006} suggest separating a conventional stack into multiple stacks where each stack contains a particular class of data. 
    On the same topic, Xu \etal \cite{xu2020merr} transforms programs to reduce memory exposure time, improving the time needed for frequent memory address randomization. 
    This makes it very challenging for an attacker to ignore the key to inject executable code. 
    This strategy disrupts the predictability of program execution and mitigates certain exploits such as speculative execution.
    No work has been found that explicitly applies this strategy to \Wasm.
    %Yet, transforming \Wasm binaries inherently randomizes the memory layout.
    %Consequently, memory accesses are randomized as these binaries are further JITed to machine code in the majority of cases.
\end{strategy}

\begin{strategy}[ISA Randomization and Simulation]
    \label{isa_rand}

    This strategy involves using a key to cypher the original program binary into another encoded binary. 
    Once encoded, the program can only be decoded at the target client, or it can be interpreted in the encoded form using a custom virtual machine implementation. 
    This technique is strong against attacks involving code inspection. 
    Kc \etal \cite{Kc03}, and Barrantes \etal \cite{barrantes2003randomized} proposed seminal works on instruction-set randomization 
    to create a unique mapping between artificial CPU instructions and real ones.
    On the same topic, Chew and Song \cite{Chew02mitigatingbuffer} target operating system randomization. They randomize the interface between the operating system and the user applications.
    Courouss{\'e} \etal~\cite{courousse2016runtime} implement an assembly-like DSL to generate equivalent code at runtime in order to increase protection against side-channel attacks. 
    Their technique generates a different program during execution using an interpreter for their DSL.
    Generally, \emph{ISA randomization and simulation} usually faces a performance penalty, especially for \Wasm, due to the decoding process as shown in WASMixer evaluation \cite{wasmixer}.
\end{strategy}


\begin{strategy}[Code obfuscation]
    \label{obfusscation}
    Code obfuscation can be seen as a simplification of \emph{ISA randomization}. 
    The main difference between encoding and obfuscating code is that the former requires the final target to know the encoding key while the latter executes as is in any client. 
    Yet, both strategies aim to tackle static reverse engineering of programs.
    In the context of \Wasm, Romano \etal \cite{wobfuscator} proposed an obfuscation technique, wobfuscator, for JavaScript in which part of the code is replaced by calls to complementary \Wasm functions.
    Yet, wobfuscator targets JavaScript code, not \Wasm binaries.
    %BREWasm \cite{BREWasm}, as a generic rewriting tool, showcases how to obfuscate \Wasm binaries. 
\end{strategy}



%% BREWasm
%BREWasm \cite{rewritingtool2023} offers a comprehensive static binary rewriting framework for \Wasm and can be considered to be the most similar to \tool. 
%For instance, it can be used to model a diversification engine.
%It parses a Wasm binary into objects, rewrites them using fine-grained APIs, integrates these APIs to provide high-level ones, and re-encodes the updated objects back into a valid Wasm binary. 
%The effectiveness and efficiency of BREWasm have been demonstrated through various Wasm applications and case studies on code obfuscation, software testing, program repair, and software optimization. 
%\todo{I do not understand the following 2 sentences. What are the main differences between \tool and BREWasm? in their input? in their output? in their purpose? in their technical approach? }
%The implementation of BREWasm follows a completely different technical approach.
%In comparison with our work, the authors pointed out that our tool employs lazy parsing of Wasm. 
%\todo{not clear: did the authors of brewasm mention something about our tool? is brewasm evaluated against security cases?}
%Although they perceived this as a limitation, it is eagerly implemented to accelerate the generation of \wasm binaries.
%Additionally, our tool leverages the parser and encoder of wasmtime, a standalone compiler and interpreter for Wasm, thereby boosting its reliability and lowering its error-prone nature.

\begin{strategy}[Enumerative synthesis]
    \label{enumerative_synthesis}
    Enumerative synthesis is a fully automated and systematic approach to generate program variants.
    It examines all possible programs specific to a given language.
    The process of enumerative synthesis commences with a piece of input program, typically a basic block.
    Incrementally, using a defined grammar, it generates all programs of size $n$.
    A generated program is then checked for equivalence to the original program, either by using a test suite or a theorem solver.
    If the generated variant is proved, it is added to the variants collection.
    The procedure continues until all potential programs have been explored.
    This approach proves especially effective when the solution space is relatively small or can be navigated efficiently.
    Jacob and colleagues \cite{jacob2008superdiversifier} implemented this strategy for x86 programs.
    They named this technique superdiversification, drawing parallels to superoptimization \cite{Massalin1987}.
    Since this strategy fully explores a program's solution space, it contains the aforementioned strategies as special cases.
    The application of enumerative synthesis to \Wasm has not been explored. 
\end{strategy}


\todo{TBD: add one on AI driven ?}


\msubsection{Equivalence Checking}
\label{equivalence:checking}


Equivalence checking between program variants is a vital component for any program transformation task, ranging from checking compiler optimizations \cite{LeCompilers} to the artificial synthesis of programs discussed in this chapter. 
It proves that two pieces of code or programs are functionally equivalent \cite{churchill2019}. 
We can roughly simplify the checking process with the following property: 
two programs are deemed equivalent if they generate identical outputs. This equivalence is observed when given identical inputs from a closed collection of inputs \cite{10.1145/2594291.2594334}.
We adopt this definition of \emph{functional equivalence modulo input} throughout this dissertation. 
In Software Diversification, equivalence checking seeks to preserve the original functionality of programs while varying observable behaviors. 
Two programs, for instance, can differ statically and still compute the same result. 
We outline two methods to check variant equivalence: by construction and prove-driven equivalence checking.


\begin{checking}[Equivalence checking by construction]
    \label{check_by_construction}
    The equivalence property is often guaranteed by construction. 
    Cleemput \etal \cite{Cleemput2012} and Homescu \etal \cite{homescu2013profile}, for example, design their transformation strategies to generate semantically equivalent program variants. 
    However, developer errors can occur in this process, necessitating further validation. 
    The test suite of the original program can serve as a check for the variant. 
    If the program variant passes the test suite \cite{harrand2020java}, it can be considered equivalent to the original. 
    However, this technique is limited by the need for a preexisting test suite and does not give guarantees. 
    An alternative method for checking program equivalence involves the use of fuzzers \cite{zalewski2017american}.
    Fuzzers randomly generate inputs that yield different observable behaviors. 
    If two inputs produce a different output in the variant, the variant and the original program are not equivalent. 
    The primary limitation for fuzzers is that the process is notably time-consuming and necessitates manual introduction of oracles.
    Recent advances in machine learning have prompted researchers to investigate the use of neural networks for verifying program equivalence.
    One such example is the work of Zhang and colleagues \cite{2023arXiv230514591Z}, which produces reference oracles and test cases by using Large Language Models.
    Although this method appears effective, it only achieves an accuracy rate of 88\%, which is insufficient for security applications and total verification.
\end{checking}

\begin{checking}[Prove-driven checking]
    \label{check_by_smt}
    In the absence of a test suite or a technique that inherently implements the equivalence property, the works mentioned earlier use theorem solvers (SMT solvers) \cite{SMT_solver} to prove equivalence of program variants. 
    The central idea for SMT solvers is to convert the two code variants into mathematical formulas. 
    The SMT solver then checks for counter-examples. 
    When it finds a counter-example, there is an input for which the two mathematical formulas yield different outputs. 
    The primary limitation of this technique is that not all algorithms can be translated into a mathematical formula, such as loops. 
    Nevertheless, this technique is frequently used for checking no-branching-programs like basic block and peephole replacements \cite{SuperoptimizationScaling}.
\end{checking}



\msubsection{Variants deployment}
Program variants, once generated and verified, may be utilized in two primary scenarios: Randomization or Multivariant Execution (MVE) \cite{jackson}. 


\begin{strategy}[Randomization]
    \label{randomization}
    In the context of our work, the term \emph{Randomization} denotes a program's ability to present different variants to different clients. 
    In this setup, a program, chosen from a collection of variants (referred to as the program's variant pool), is assigned to a random client during each deployment. 
    Jackson \etal \cite{jackson} define the variant pool in Randomization as herd immunity, as vulnerable binaries can only affect a segment of the client community. 
    El-Khalil and colleagues \cite{ElKhalil2004} suggest employing a custom compiler to generate varying binaries from the compilation process. 
    They adapt a version of GCC 4.1 to partition a conventional stack into several component parts, termed multistacks. 
    Similarly, Singhal and colleagues, propose Cornucopia \cite{cornucopia}.
    Cornucopia generates multiple variants of a program by using different compiler flag combinations.
    Aga and colleagues \cite{aga2019smokestack}, contributing to this discussion, propose the generation of program variants through the randomization of its data layout in memory. 
    This method allows each variant to operate on the same data in memory but at different memory offsets. 
    Randomization can also be applied to virtual machines and operating systems. On this note, Kc \etal \cite{Kc03} establish a unique mapping between artificial CPU instructions and actual ones, enabling the assignment of various variants to specific target clients. 
    In a similar vein, Xu \etal \cite{xu2020merr} recompile the Linux Kernel to minimize the exposure time of persistent memory objects, thereby increasing the frequency of address randomization.
\end{strategy}


\begin{strategy}[Multivariant Execution (MVE)]
    Multiple program variants are composed into a single binary, known as a multivariant binary \cite{cox06}. 
    Each multivariant binary is randomly deployed to a client.
    Then, the multivariant binary executes its embedded program variants at runtime. 
    These embedded variants can either execute in parallel to check for inconsistencies, or as a single program to randomize execution paths \cite{bhatkar03}. 
    Bruschi and colleagues extend the concept of executing two variants in parallel, introducing non-overlapping and randomized memory layouts \cite{bruschi2007diversified}. 
    At the same time, Salamat \etal modifies a standard library to generate 32-bit Intel variants. 
    These variants have a stack that grows in the opposite direction, allowing for the detection of memory inconsistencies \cite{salamat2007stopping}. 
    Davi and colleagues propose Isomeron, an approach for execution-path randomization \cite{davi2015isomeron}. 
    Isomeron operates by simultaneously loading the original program and a variant. 
    It then uses a coin flip to determine which copy of the program to execute next at the function call level. 
    Previous works have highlighted the benefits of limiting execution to only two variants in a multivariant environment. 
    Agosta and colleagues, as well as Crane and colleagues, used more than two generated programs in the multivariant composition, thereby randomizing software control flow at runtime \cite{agosta2015meet, crane2015thwarting}. 
    Both strategies have proven effective in enhancing security by addressing known vulnerabilities, such as Just-In-Time Return-Oriented Programming (JIT-ROP) attacks \cite{jackson2011compiler} and power side-channel attacks \cite{amarilli2011can}. 
    Lastly, only Voulimeneas \etal \cite{voulimeneas2021dmvx} have recently proposed a multivariant execution system that enhances security by parallelizing the execution of variants across different machines.
\end{strategy}

\msubsection{Software Diversification Asssesment}

Assessing software diversification poses a significant challenge. 
The size of the variant space does not necessarily correlate with a variant's ability to evade attacks \cite{cohen1993operating}. 
For example, consider two \Wasm program variants that differ only in their instruction order. 
Despite their static differences, JIT compilers effectively optimize both, resulting in identical machine code during execution. 
In this scenario, the variant space may be extensive, but the variants lack diversity. 
Ideally, real attacks provide the most accurate assessment of diversification. 
However, this approach is not always feasible, as it entails the real-time deployment and testing of numerous variants. 
Thus, a combination of static and dynamic metrics is necessary when generating and assessing software diversification.

\begin{strategy}[Static based assesment]
    \label{static_based}
\end{strategy}

\begin{strategy}[Dynamic based assesment]
    \label{trace_based}
\end{strategy}


\msubsection{Offensive Diversification}
\label{offensive_definition}
Lundquist and colleagues \cite{offensive_div} distinguish Software Diversification into two categories: Defensive and Offensive Diversification. 
On the one hand, Defensive Software Diversification introduces unpredictability in system behavior. 
By making software less predictable, defensive software diversification aims to proactively deter attacks, acting as a complementary strategy to other, more reactive, security measures. 
The majority of previously discussed works in this section contribute to defensive diversification.
Yet, Software Diversification that aims to create diverse harmful programs is considered Offensive Diversification \cite{fred1986computer}.


\begin{strategy}[Offensive Diversification]   
    Offensive Diversification is conceptually equal to Defensive Software Diversification.
    Yet, in an offensive context, one may apply diversification techniques to malware or other malicious codes to evade detection by security software \cite{8714698}.
    For example, one might equate Offensive Diversification with Code obfuscation, if its purpose shifts from preventing reverse engineering by malicious actors, to evading detection by malware analysis systems.
    
\end{strategy}


Malicious actors may employ previously discussed diversification strategies to evade detection, including genetic programming \cite{castro2019aimed}.
Over time, these evasion techniques have evolved in both complexity and sophistication \cite{Aghakhani2020WhenMI}.
Chua \etal \cite{chua}, for instance, suggested a framework for automatically obfuscating the source code of Android applications using method overloading, opaque predicates, try-catch, and switch statement obfuscation, resulting in multiple versions of identical malware.
Moreover, machine learning approaches have been utilized to develop evasive malware \cite{2021arXiv211111487D}, drawing on a corpus of pre-existing malware \cite{Bostani2021EvadeDroidAP}.
These methods aim to thwart static malware detectors, yet, more advanced techniques focus on evading dynamic detection mostly by employing throttling \cite{Lu2013WeaknessesID, payer2014embracing}.


The term Offensive Software Diversification may appear counterintuitive.
Yet, such approaches measure the resilience and accuracy of security systems. 
This is an almost unexplored area in \Wasm, posing a threat to malware detection accuracy. 
Specifically, only Bahnsali \etal's seminal work\cite{10.1145/3507657.3528560} has demonstrated that a cryptomining algorithm's source code can evade pre-existing malware detection methods. 
More recently, Madvex \cite{madvex} has sought to obfuscate \Wasm binaries to achieve malware evasion, but this approach is limited to altering only the code section of \Wasm binaries.


\msubsection{Open challenges}
\label{sota:openchallenges}
As outlined in \autoref{background:wasm:challenges}, our primary motivation for the contributions of this thesis is the open issues within the \Wasm ecosystem. 
We see potential in employing Software Diversification to address them. 
Based on our previous discussion, we highlight several open challenges in the realm of Software Diversification for \Wasm. 
First, \Wasm, being an emerging technology, is still in the process of implementing defensive measures \cite{Stevienart paper here}. 
The process of officially adopting a new defensive measure is inherently slow, making software diversification a potentially valuable preemptive strategy. 
Second, despite the abundance of related work on software diversity, its exploration in the context of \Wasm remains limited. 
Third, both randomization and multivariant execution have been largely unexplored. 
Lastly, the works on malware detection discussed in \autoref{background:wasm:analysis} suggest that offensive diversification could be useful in measuring the resilience and accuracy of security systems for \Wasm.





% Besides, the process of diversifying a \Wasm program can be conceptualized as a three-stage procedure: parsing the program, transforming it, and finally re-encoding it back into \wasm. 
%Our review of the literature has revealed several studies that have employed parsing and encoding components for \wasm binaries across various domains as discussed in \autoref{background:wasm:analysis}. 
%This indicates that these works accept a \wasm binary as an input and output a unique \wasm binary. 
%When the transformation stage introduces randomized mutations to the original program, the aforementioned tools could potentially be built as diversifiers.
%However, we have not found any work that evaluates the impact of such tools as diversifiers.

