

\msection{Software diversification}
\label{sota:sw}

%%ORIGINAL
Software diversification involves the synthesis, reuse, distribution, and execution of different, functionally equivalent programs.
Along with this work, we refer to those programs as software variants.
As outlined in Baudry \etal's survey \cite{natural_diversity}, Software Diversification falls into five usage categories: reusability \cite{pohl2005software},performance \cite{10.1145/2025113.2025133}, fault tolerance \cite{1659219}, software testing \cite{Chen2010AdaptiveRT}, and security \cite{cohen1993operating}. 
Our work specifically contributes to the last two categories.
Based on the works of Cohen \etal \cite{cohen1993operating}, Forrest \etal \cite{595185}, Jackson \etal \cite{jackson} and Baudry \etal \cite{natural_diversity}, this section presents core concepts and related works.
Notice that, we do not regard program variants from Software Product Lines \cite{10.1145/2580950} as instances of software diversification.
The primary reason is that, by design, Software Product Lines do not produce functionally equivalent programs. 


Software variants in our context refer to functionally equivalent versions of an original program, produced through Software Diversification at various stages of the software lifecycle, from dependencies (coarse-grained) to machine code levels (fine-grained).
The main goal of Software Diversification is to increase the cost of exploitation by making software less predictable. 
Diversification may be natural \cite{natural_diversity} or automatic \cite{offensive_div}. 
Natural diversity refers to the side effect of humans creating software variants using different programming languages, compilers, and operating systems \cite{natural_diversity}, all of which adhere to the initial requirements. 
The software market and competition typically address the creation of natural diversity. 
For example, Firefox and Chrome web browsers demonstrate natural diversity due to their practical differences in implementation and performance, despite serving the same purpose. 
This logic extends to operating systems, database engines, virtual machines, and application servers \cite{natural_diversity}. 
Natural diversity significantly aids in system security, as different variants are not susceptible to the same vulnerabilities \cite{781031, 10.5555/1009382.1009753}. 
Unlike N-Version programming \cite{6312924}, natural diversity organically emerges over decades. 
In other words, while it does not require the allocation of additional human effort, natural diversity cannot be automatically generated. 
This is because it is a side effect of the software development process.
Given that \Wasm is a relatively new technology, natural diversity is presently not a feasible option. 
Hence, for \Wasm, feasible options are systematic and automatic diversification approaches.



\msubsection{Automatic generation of software variants}
\label{artificial_diversity}

The concept of automatic software variants starts with Randell's 1975 work \cite{10.1145/390016.808467}, which put forth the notion of artificial fault-tolerant instruction blocks. 
Artificial Software Diversification, as proposed by Cohen and Forrest in the 1990s \cite{cohen1993operating, 595185}, gets its development through rewriting strategies. 
These strategies consist of sets of rewriting rules for modifying software components to create functionally equivalent, yet distinct, programs. 
Rewriting strategies typically take the form of tuples: \texttt{instr1 => (instr2, instr3, ...)}, where \texttt{instr} represents the original code and \texttt{(instr2, instr3, ...)} denotes the functionally equivalent code.


\begin{strategy}[Rewriting strategy]
    \label{rewriting_strategy}
    The automatic creation of Software Diversification begins with creating rewriting rules.
    A rewriting rule refers to a functionally equivalent substitution for a code segment, manually written. 
    These rules can be applied at varying levels, from coarse to fine-grained. 
    This can range from the program dependencies level \cite{Harrand1650630} to the instruction level \cite{offensive_div}. 
    For example, Cleemput et al.~\cite{Cleemput2012} and Homescu et al.~\cite{homescu2013profile} inject NOP instructions to yield statically varied versions at the instruction level. 
    Here, the rewriting rule is represented as \texttt{instr => (nop instr)}, signifying an insertion of a \texttt{nop} operation preceding the instruction.


    %Although the studies by Cleemput et al. and Homescu et al. are easily applicable to \Wasm, this specific strategy may fall under the \emph{non preserved} category since \Wasm typically compiles later. 
    %This implies that JIT compilers could nullify this diversification strategy by merely applying straightforward optimizations.
\end{strategy}



\begin{strategy}[Instruction Reordering]
    \label{instruction_reordering}
    This strategy reorders instructions in a program.
    For example, variable declarations may change if compilers reorder them in the symbol tables. 
    This prevents static examination and analysis of parameters and alters memory locations. 
    In this area, Bhatkar \etal \cite{bhatkar03, bhatkar2005efficient} proposed the random permutation of variable and routine order for ELF binaries.
    Such strategies are not implemented for \Wasm to the best of our knowledge.
\end{strategy}


\begin{strategy}[Adding, Changing, Removing Jumps and Calls]
    \label{jumps}
    This strategy generates program variants by adding, changing, or removing jumps and calls in the original program. 
    Cohen \cite{cohen1993operating} primarily illustrated this concept by inserting random jumps in programs. Pettis and Hansen \cite{pettisochhansen} suggested splitting basic blocks and functions for the PA-RISC architecture, inserting jumps between splits.
    Similarly, Crane \etal~\cite{crane2015thwarting} de-inlined basic blocks of code as an LLVM pass. 
    In their approach, each de-inlined code transforms into semantically equivalent functions that are randomly selected at runtime to replace the original code calculation. 
    On the same topic, Bhatkar \etal~\cite{bhatkar2005efficient} extended their previous approach \cite{bhatkar03}, replacing function calls with indirect pointer calls in C source code, allowing post-binary reordering of function calls. 
    In the \Wasm context, thesimilar work is Wobfuscator \cite{wobfuscator}.
    Wobfuscator, a JavaScript obfuscator, substitutes pieces of JavaScript code with \Wasm code, e.g., numeric calculi.
    This strategy effectively uses the interleaving of calls between JavaScript and \Wasm to provide JavaScript variants.
\end{strategy}


\begin{strategy}[Program Memory and Stack Randomization]
    \label{mem_strategy}
    This strategy alters the layout of programs in the host memory. 
    Additionally, it can randomize how a program variant operates its memory. 
    The work of Bhatkar \etal \cite{bhatkar03, bhatkar2005efficient} proposes to randomize the base addresses of applications and library memory regions in ELF binaries. 
    Tadesse Aga and Autin \cite{aga2019smokestack}, and Lee \etal \cite{lee2021savior} propose a technique to randomize the local stack organization for function calls using a custom LLVM compiler.
    Younan \etal \cite{Younan2006} suggest separating a conventional stack into multiple stacks where each stack contains a particular class of data. 
    On the same topic, Xu \etal \cite{xu2020merr} transforms programs to reduce memory exposure time, improving the time needed for frequent memory address randomization. 
    This makes it very challenging for an attacker to ignore the key to inject executable code. 
    This strategy disrupts the predictability of program execution and mitigates certain exploits such as speculative execution.
    No work has been found that explicitly applies this strategy to \Wasm.
    %Yet, transforming \Wasm binaries inherently randomizes the memory layout.
    %Consequently, memory accesses are randomized as these binaries are further JITed to machine code in the majority of cases.
\end{strategy}

\begin{strategy}[ISA Randomization and Simulation]
    \label{isa_rand}

    This strategy involves using a key to cipher the original program binary into another encoded binary. 
    Once encoded, the program can only be decoded at the target client, or it can be interpreted in the encoded form using a custom virtual machine implementation. 
    This technique is strong against attacks involving code inspection. 
    Kc \etal \cite{Kc03}, and Barrantes \etal~\cite{barrantes2003randomized} proposed seminal works on instruction-set randomization 
    to create a unique mapping between artificial CPU instructions and real ones.
    On the same topic, Chew and Song \cite{Chew02mitigatingbuffer} target operating system randomization. They randomize the interface between the operating system and the user applications.
    Courouss{\'e} \etal~\cite{courousse2016runtime} implement an assembly-like DSL to generate equivalent code at runtime in order to increase protection against side-channel attacks. 
    Their technique generates a different program during execution using an interpreter for their DSL.
    Generally, \emph{ISA randomization and simulation} usually faces a performance penalty, especially for \Wasm, due to the decoding process as shown in WASMixer evaluation \cite{wasmixer}.
\end{strategy}


\begin{strategy}[Code obfuscation]
    \label{obfusscation}
    Code obfuscation can be seen as a simplification of \emph{ISA randomization}. 
    The main difference between encoding and obfuscating code is that the former requires the final target to know the encoding key while the latter executes as is in any client \cite{10.1145/3176258}. 
    Yet, both strategies aim to tackle static reverse engineering of programs.
    In the context of \Wasm, Romano \etal \cite{wobfuscator} proposed an obfuscation technique, wobfuscator, for JavaScript in which part of the code is replaced by calls to complementary \Wasm functions.
    Yet, wobfuscator targets JavaScript code, not \Wasm binaries.
    %BREWasm \cite{BREWasm}, as a generic rewriting tool, showcases how to obfuscate \Wasm binaries. 
\end{strategy}



\begin{strategy}[Enumerative synthesis]
    \label{enumerative_synthesis}
    Enumerative synthesis is a fully automated and systematic approach to generate program variants.
    It examines all possible programs specific to a given language.
    The process of enumerative synthesis commences with a piece of input program, typically a basic block.
    Incrementally, using a defined grammar, it generates all programs of size $n$.
    A generated program is then checked for equivalence to the original program, either by using a test suite or a theorem solver.
    If the generated variant is proven equivalent, it is added to the variant's collection.
    The procedure continues until all potential programs have been explored.
    This approach proves especially effective when the solution space is relatively small or can be navigated efficiently.
    Jacob \etal \cite{jacob2008superdiversifier} implemented this strategy for x86 programs.
    They named this technique superdiversification, drawing parallels to superoptimization \cite{Massalin1987}.
    Since this strategy fully explores a program's solution space, it contains the aforementioned strategies as special cases.
    The application of enumerative synthesis to \Wasm has not been explored. 
\end{strategy}


\msubsection{Equivalence Checking}
\label{equivalence:checking}


Equivalence checking between program variants is a vital component for any program transformation task, ranging from checking compiler optimizations \cite{LeCompilers} to the artificial synthesis of programs discussed in this chapter. 
It proves that two pieces of code or programs are functionally equivalent \cite{churchill2019}. 
We can roughly simplify the checking process with the following property: 
two programs are deemed equivalent if they generate identical outputs when given identical inputs from a closed collection of inputs \cite{10.1145/2814270.2814319}.
We adopt this definition of \emph{functional equivalence modulo input} throughout this dissertation. 
In Software Diversification, equivalence checking seeks to preserve the original functionality of programs while varying observable behaviors. 
Two programs, for instance, can differ statically and still compute the same result. 
We outline three methods to check variant equivalence: by construction, check modulo tests and proof-driven equivalence checking.


\begin{checking}[Equivalence by construction]
    \label{check_by_construction}
    The equivalence property can be guaranteed by construction.
    As previously mentioned, Cleemput \etal \cite{Cleemput2012} and Homescu \etal \cite{homescu2013profile} exemplify transformation strategies that generate semantically equivalent program variants. 
    These variants are equivalent by construction. 
    In their case, NOP instructions produce statically different variants. 
    NOP operations, interleaved by any other type of original instruction, serve as a functionally equivalent replacement. 
    However, developer errors may occur during this process, necessitating further validation.
    The test suite of the original program can serve as a check for the variant. 
\end{checking}

\begin{checking}[Checking modulo tests]
    \label{check_by_tests}

    The process of checking modulo tests involves utilizing a test suite to confirm the equivalence of program variants \cite{10.1007/s10710-013-9195-8, 10.1145/2610384.2610415}. 
    When a program variant successfully passes the test suite, it is deemed equivalent to the original. 
    It is reasonable to assume that projects prioritizing quality and security are likely to have a robust test suite that facilitates this type of equivalence checking.
    However, this technique's effectiveness is limited by the necessity for a preexisting test suite. 
    Yet, as an alternative, fuzzers can be used to automatically generate tests \cite{zalewski2017american}.
    Fuzzers operate by randomly generating inputs that lead to different observable behaviors. 
    If a variant produces a different output from two identical inputs, it is not equivalent to the original program. 
    Fuzzers' primary drawback is their time-consuming nature and the requirement for manually introducing oracles.
    Recent advancements in the field of machine learning have led researchers to explore the application of neural networks in verifying program equivalence.
    Zhang and his team's work provides an example of this, where Large Language Models are used to generate reference oracles and test cases \cite{2023arXiv230514591Z}.
    Despite its effectiveness, this method attains an accuracy rate of just 88\%, which falls short of providing complete verification.

\end{checking}

\vspace{1em}

\begin{checking}[Formal verification]
    \label{check_by_smt}
    In the absence of a test suite or a technique that inherently implements the equivalence property, the works mentioned earlier use automated theorem provers.
    Theorem provers rely on SMT solvers \cite{SMT_solver} to prove the equivalence of program variants. 
    The central idea for theorem provers is to convert the two code variants into mathematical formulas. 
    The core component, the SMT solver, then checks for counter-examples that satisfy the negation of the mathematical formulas \cite{kesseli2018counterexample}.
    When it finds a counter-example, it uncovers an input for which the two mathematical formulas yield different outputs. 
    The primary limitation of this technique resides in the conversion process.    
    All algorithms can be translated into a mathematical formula. 
    However, under certain theories such as loops for linear arithmetic, the satisfiability query may be undecidable. 
    As a result, SMT solvers cannot make a decision.
    Nevertheless, this technique is frequently used for checking no-jump-programs like basic block and peephole replacements \cite{SuperoptimizationScaling}.
\end{checking}



\msubsection{Variants deployment}
\label{deployment}
Program variants, once generated and verified, may be used in two primary scenarios: Randomization or Multivariant Execution (MVE) \cite{jackson}. 


\begin{strategy}[Randomization]
    \label{randomization}
    In the context of our work, the term \emph{Randomization} denotes a program's ability to present different variants to different clients. 
    In this setup, a program, randomly chosen from a collection of variants (referred to as the program's variant pool), is assigned to a the client during each deployment. 
    Jackson \etal \cite{jackson} define the variant pool in Randomization as herd immunity, as vulnerable binaries can only affect a segment of the client community. 
    El-Khalil \etal \cite{ElKhalil2004} suggest employing a custom compiler to generate varying binaries from the compilation process. 
    They adapt a version of GCC 4.1 to partition a conventional stack into several component parts, termed multistacks. 
    Similarly, Singhal \etal, propose Cornucopia \cite{cornucopia}.
    Cornucopia generates multiple variants of a program by using different compiler flag combinations.
    Aga \etal propose the generation of program variants through the randomization of its data layout in memory\cite{aga2019smokestack}. 
    This method allows each variant to operate on the same data in memory but at different memory offsets. 
    Randomization can also be applied to virtual machines and operating systems. On this note, Kc \etal \cite{Kc03} establish a unique mapping between artificial CPU instructions and actual ones, enabling the assignment of various variants to specific target clients. 
    In a similar vein, Xu \etal \cite{xu2020merr} recompile the Linux Kernel to minimize the exposure time of persistent memory objects, thereby increasing the frequency of address randomization.
\end{strategy}


\begin{strategy}[Multivariant Execution (MVE)]
    Multiple program variants are composed into a single binary, known as a multivariant binary \cite{cox06}. 
    Each multivariant binary is randomly deployed to a client.
    Then, the multivariant binary executes its embedded program variants at runtime. 
    These embedded variants can either execute in parallel to check for inconsistencies, or as a single program to randomize execution paths \cite{bhatkar03}. 
    Bruschi \etal extend the concept of executing two variants in parallel, introducing non-overlapping and randomized memory layouts \cite{bruschi2007diversified}. 
    At the same time, Salamat \etal~modify a standard library to generate 32-bit Intel variants. 
    These variants have a stack that grows in the opposite direction, allowing for the detection of memory inconsistencies \cite{salamat2007stopping}. 
    Davi \etal propose Isomeron, an approach for execution-path randomization \cite{davi2015isomeron}. 
    Isomeron operates by simultaneously loading the original program and a variant. 
    It then uses a coin flip to determine which copy of the program to execute next at the function call level. 
    Previous works have highlighted the benefits of limiting execution to only two variants in a multivariant environment. 
    Agosta \etal, as well as Crane \etal, used more than two generated programs in the multivariant composition, thereby randomizing software control flow at runtime \cite{agosta2015meet, crane2015thwarting}. 
    Both strategies have proven effective in enhancing security by addressing known vulnerabilities, such as Just-In-Time Return-Oriented Programming (JIT-ROP) attacks \cite{jackson2011compiler} and power side-channel attacks \cite{amarilli2011can}. 
    Lastly, only Voulimeneas \etal \cite{voulimeneas2021dmvx} have recently proposed a multivariant execution system that enhances security by parallelizing the execution of variants across different machines.
\end{strategy}


\msubsection{Measuring Software Diversification}
\label{measuring_diversification}
Measuring Software Diversification presents a significant challenge. 
The size of the variant space does not necessarily correlate with a variant's capacity to fulfill an objective such as hardening attacks by making systems less predictable~\cite{cohen1993operating}. 
Ideally, real scenarios would provide the most accurate measurement of diversification, e.g., demonstrating a variant's effectiveness under specific attacks. 
However, such an approach is not always feasible, since Software Diversification is a preventive strategy. 
Hence, a combination of static and dynamic metrics is required for measuring Software Diversification.

\begin{strategy}[Static comparison of variants]
    \label{static_based}
    Static metrics are used to measure the diversity of programs without needing execution. 
    The fundamental concept entails comparing variant source codes or binary codes to determine how diverse they are. 
    Usually, comparing variants means defining a distance metric between programs \cite{10.1145/2814270.2814319} where the more different the programs are, the greater the distance. 
    At the low-level of bytecode instructions, for example, these metrics include counting instructions \cite{10.1007/978-3-642-00730-9_10}, Levenshtein distance \cite{DBLP:journals/corr/abs-2111-09934}, and global alignments \cite{CROW}. 
    On the other hand, at the high-level of source code, these metrics often rely on Abstract Syntax Tree (AST) diffing, such as GUMtree-based distances \cite{gumtree} or machine learning inference \cite{203634}. 
    As an example of measuring the diversification, Bostani \etal~\cite{Bostani2021EvadeDroidAP} illustrate the use of static distances in guiding the generation process of variants. 
    They categorize the space of Android applications into malware and goodware. 
    Then, they create malware variants by employing a static distance metric to approach the goodware group as closely as possible, thus successfully evading malware classifiers.
\end{strategy}

\begin{strategy}[Dynamic comparison of variants]
    \label{trace_based}
    Static comparisons between variants inherently have limitations. 
    For example, two variants may show differences at the source code level but exhibit identical behavior during execution. 
    Take the addition of \texttt{nop} operations to a program as an instance. 
    Despite source code level differences, the variant and the original program execute identical instructions, leading to similar behaviors modulo input. 
    Measuring Software Diversification primarily aims to demonstrate variant-specific observabilities. 
    While static differences are usually observable, runtime information holds complementary relevance \cite{yao2018anomaly}. 
    Therefore, dynamic metrics are essential to assess the diversity of variants. 
    For instance, Forrest \etal \cite{forrest_system_call} were pioneers in classifying program behaviors by analyzing their system call traces using n-grams profiling. 
    Cabrera \etal used a global alignments approach to gauge the diversity of JavaScript bytecode traces within the Chrome browser \cite{STRAC}. 
    Fang \etal proposed a method to counteract JavaScript obfuscation techniques used in malicious code, by analyzing dynamic information captured from V8 bytecode traces \cite{8482113}. 
    Dynamic metrics are primarily employed to cluster similar behaviors.
    Following the same logic, the diversity is greater when the difference between behaviors is larger. 
    Notice that, dynamic metrics can be difficult due to the expense of program execution or the complication of required user interaction. 
    On the other hand, malware programs, which usually do not require user interaction, are simpler to evaluate in controlled environments before actual deployment.

\end{strategy}

In the context of \Wasm, there exist no explicit works on Software Diversification.
Consequently, previous metrics have not been directly applied to measure diversification in \Wasm binaries.
However, in other domains, such as the analysis of \Wasm binaries, several studies have employed static metrics.
For example, VeriWasm quantifies attack-based patterns, stating that a \Wasm binary is more secure with a lower pattern count \cite{veriwasm}.
This metric might potentially serve as a guide during variant generation.
In the field of malware detection, MINOS \cite{MINOS} proposes transforming \Wasm binaries into grayscale images.
They then employ convolutional neural networks to identify malware, where an increased similarity to a malware image increases the probability of the binary being malware.
Regarding the dynamic comparisons, Wang \etal's study \cite{SEISMIC} profiles \Wasm instructions during runtime to identify malicious behavior.


\msubsection{Offensive or Defensive assessment of diversification}
\label{offensive_definition}

Lundquist \etal~\cite{offensive_div} distinguish Software Diversification into two categories: Defensive and Offensive Diversification. 
On the one hand, Defensive Software Diversification introduces unpredictability in system behavior. 
By making software less predictable, defensive Software Diversification aims to proactively deter attacks, acting as a complementary strategy to other, more reactive, security measures. 
The majority of previously discussed works in this section contribute to defensive diversification.
Yet, Software Diversification that aims to create diverse harmful programs is considered Offensive Diversification \cite{fred1986computer}.


\begin{strategy}[Offensive Diversification]   
    Offensive Diversification is conceptually equal to Defensive Software Diversification.
    Yet, in an offensive context, one may apply diversification techniques to malware or other malicious codes to evade detection by security software \cite{8714698}.
    One might equate Offensive Diversification with Code obfuscation, if its purpose shifts from preventing reverse engineering by malicious actors, to evading detection by malware analysis systems.
    
    
\end{strategy}


Malicious actors may employ previously discussed diversification strategies to evade detection \cite{castro2019aimed}.
For instance, in the Web context, Weihang \etal propose to randomly transform HTML elements of web pages to evade advertisement blockers \cite{webranz}.
Over time, evasion techniques have evolved in both complexity and sophistication \cite{Aghakhani2020WhenMI}.
Chua \etal \cite{chua}, for instance, suggested a framework for automatically obfuscating the source code of Android applications using method overloading, opaque predicates, try-catch, and switch statement obfuscation, resulting in multiple versions of identical malware.
Moreover, machine learning approaches have been used to develop evasive malware \cite{2021arXiv211111487D}, drawing on a corpus of pre-existing malware \cite{Bostani2021EvadeDroidAP}.
These methods aim to thwart static malware detectors, yet, more advanced techniques focus on evading dynamic detection mostly by employing throttling \cite{Lu2013WeaknessesID, payer2014embracing}.


The term Offensive Software Diversification may seem counterintuitive.
Yet, such approaches measure the resilience and accuracy of security systems. 
This is an almost unexplored area in \Wasm, posing a threat to malware detection accuracy. 
Specifically, only Bhansali \etal's seminal work\cite{10.1145/3507657.3528560} has demonstrated that a cryptomining algorithm's source code can evade pre-existing malware detection methods. 
More recently, Madvex \cite{madvex} has sought to obfuscate \Wasm binaries to achieve malware evasion, but this approach is limited to altering only the code section of \Wasm binaries.



\msection{Open challenges for Software Diversification}
\label{sota:openchallenges}
As outlined in \autoref{background:wasm:challenges}, our primary motivation for the contributions of this thesis are the open issues within the \Wasm ecosystem. 
We see potential in employing Software Diversification to address them. 
Based on our previous discussion, we highlight several open challenges in the realm of Software Diversification for \Wasm. 
First, \Wasm, being an emerging technology, is in the process of implementing defensive measures. 
In addition, while measures for \Wasm can be standardized, the implementation of these standards across the ecosystem is naturally slow. 
Therefore, applying Software Diversification directly to the generation of \Wasm binaries, according to any given specification, could serve as a valuable strategy to lessen the impact of vulnerabilities.
Second, despite the abundance of related work on software diversity, its exploration in the context of \Wasm remains limited. 
This thesis is the first to investigate Software Diversity in depth for the emerging \Wasm ecosystem.
Third, both randomization and multivariant execution remain largely unexplored within the \Wasm context. 
The deployment of Software Diversification in \Wasm poses unique challenges. 
\Wasm ecosystems are remarkably dynamic. 
Web browsers and FaaS platforms serve as prime examples. 
In these environments, \Wasm binaries are served millions of times simultaneously to the former, while new \Wasm binaries are cold-spawned and executed upon each user request in the latter. 
Thus, designing practical Software Diversification for \Wasm requires careful consideration of the deployment environment. 
Last but not least, research on malware detection, as discussed in \autoref{background:wasm:analysis}, suggests that offensive diversification may assist in evaluating the resilience and accuracy of \Wasm's security systems.

