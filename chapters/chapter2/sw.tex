

\msection{Software diversification}
\label{sota:sw}



This dissertation presents toolsets, approaches and methodologies designed to enhance \Wasm security proactively through Software Diversification.
First, Software Diversification could expand the capabilities of the mentioned tools by incorporating diversified program variants, making it more challenging for attackers to exploit any missed vulnerabilities.
Generated as proactive security, these diversified variants can simulate a broader set of real-world conditions, thereby making \Wasm analysis tools more accurate. 
Second, we noted that current solutions to mitigate side-channel attacks on \Wasm binaries are either specific to certain attacks or need the modification of runtimes, e.g., Swivel as a cloud-deployed compiler.
Software Diversification could mitigate yet-unknown vulnerabilities on \Wasm binaries by generating diversified variants in a platform-agnostic manner.


Software Diversification has been extensively studied in recent decades. 
This section explores its current state of the art. 
Software diversification involves the synthesis, reuse, distribution, and execution of different, functionally equivalent programs. 
As outlined in Baudry \etal's survey \cite{natural_diversity}, software diversification falls into five usage categories: reusability \cite{pohl2005software}, software testing \cite{Chen2010AdaptiveRT}, performance \cite{10.1145/2025113.2025133}, fault tolerance \cite{1659219}, and security \cite{cohen1993operating}. 
Our work specifically contributes to the last two categories.
This section presents related works, emphasizing how they generate diversification and apply it to \Wasm.


% - \todo{Quantifying Cybersecurity Effectiveness of Dynamic Network Diversity} \cite{9524529}

% - \todo{Sharing is caring: secure and efficient shared memory support for MVEEs} \cite{10.1145/3492321.3519558}

% - \todo{For fault tolerance: System structure for software fault tolerance} \cite{10.1145/390016.808467}

\msubsection{Generation of Software Variants}

Software variants are functionally equivalent versions of an original program, created through software diversification at different stages of the software lifecycle, such as the source code or machine code levels. 
The diversification can be either natural \cite{natural_diversity}, arising spontaneously but requiring significant human effort, or artificial \cite{offensive_div}, which is automated and the focus of our work in the context of \Wasm.
The concept of software variants dates back to Randell's 1975 work \cite{10.1145/390016.808467}, which introduced the idea of fault-tolerant instruction blocks. Later, artificial software diversification was further developed through rewriting strategies, as proposed by Cohen and similarly by Forrest in the 1990s \cite{cohen1993operating, 595185}. 
These strategies consist of rule sets for altering software components to create functionally equivalent but distinct programs. 
Our work builds on these foundational studies and focuses on artificial diversification techniques, particularly for \Wasm, drawing insights from significant works by Baudry \etal \cite{natural_diversity} and Jackson \etal \cite{jackson}.
In the following, we group the major strategies used to artificially generate software variants and their current state with respect to \Wasm.


\wrule{Replacement of Equivalent Instructions:}
One can replace sections of programs with semantically equivalent code. 
This method requires substituting the original code with identical arithmetic expressions or injecting instructions that do not alter the final computation outcome. 
There are primarily two methods for generating such equivalent code: rewriting rules and enumerative synthesis. 
In the first method, manual rewriting rules dictate the replacement strategies. 
A rewriting rule consists of a code segment and its semantically identical substitution. 
For instance, Cleemput et al. \cite{Cleemput2012} and Homescu et al.~\cite{homescu2013profile} introduce NOP instructions to produce statically varied versions. 
In these studies, the rewriting rule is expressed as \texttt{instr => (nop instr)}, implying a \texttt{nop} operation followed by the instruction as a valid substitute.
In contrast, enumerative synthesis explores all potential programs specific to a language. 
In this domain, Jacob et al. \cite{jacob2008superdiversifier} introduced a technique called superdiversification for x86 binaries. 
Similarly, Tsoupidi et al. \cite{Tsoupidi2020ConstraintBasedSD} presented Diversity by Construction, a constraint-based compiler that creates software diversity for the MIPS32 architecture.  
Their technique employs a constraint solver to generate program variants that are semantically equivalent by design.
Compared to other methods, Jacob et al. and Tsoupidi et al.'s work does not need manually written replacement strategies, but their reach is limited by theorem solvers.
While their techniques can be implemented in any language, they are not directly applicable to \Wasm.
For instance, while the studies of Cleemput et al. and Homescu et al. are directly applicable to \Wasm, since \Wasm typically compiles later, this specific strategy could fall into a \emph{non preserved} category, meaning JIT compilers could eliminate this diversification strategy by simply applying straightforward optimizations.
Conversely, the application of enumerative synthesis to \Wasm has not been explored, and it is actually one of our contributions. 


\wrule{Instruction Reordering:} 
This strategy involves reordering independent instructions or entire program blocks.
The location of variable declarations may also change if compilers reorder them in the symbol tables. This prevents static examination and analysis of parameters and alters memory locations. In this area, Bhatkar \etal \cite{bhatkar03, bhatkar2005efficient} proposed the random permutation of variable and routine order for ELF binaries.
Such strategies are not implemented for \Wasm to the best of our knowledge.

\wrule{Adding, Changing, Removing Jumps and Calls:}
This strategy generates program variants by adding, changing, or removing jumps and calls in the original program. Cohen \cite{cohen1993operating} primarily illustrated this concept by inserting random jumps in programs. Pettis and Hansen \cite{pettisochhansen} suggested splitting basic blocks and functions for the PA-RISC architecture, inserting jumps between splits.
Similarly, Crane \etal~\cite{crane2015thwarting} de-inlined basic blocks of code as an LLVM pass. 
In their approach, each de-inlined code transforms into semantically equivalent functions that are randomly selected at runtime to replace the original code calculation. 
On the same topic, Bhatkar \etal \cite{bhatkar2005efficient} extended their previous approach \cite{bhatkar03}, replacing function calls with indirect pointer calls in C source code, allowing post-binary reordering of function calls. 
Recently, Romano \etal \cite{wobfuscator} proposed an obfuscation technique for JavaScript in which part of the code is replaced by calls to complementary Wasm functions.
As previously discussed in \autoref{wasm:control_flow}, the control flow of \Wasm is restricted, making them impractical to directly port these strategies to \Wasm.

\wrule{Program Memory and Stack Randomization:}
This strategy alters the layout of programs in the host memory. 
Additionally, it can randomize how a program variant operates its memory. 
The work of Bhatkar \etal \cite{bhatkar03, bhatkar2005efficient} proposes to randomize the base addresses of applications and library memory regions in ELF binaries. 
Tadesse Aga and Autin \cite{aga2019smokestack}, and Lee \etal \cite{lee2021savior} propose a technique to randomize the local stack organization for function calls using a custom LLVM compiler.
Younan \etal \cite{Younan2006} suggest separating a conventional stack into multiple stacks where each stack contains a particular class of data. 
On the same topic, Xu \etal \cite{xu2020merr} transforms programs to reduce memory exposure time, improving the time needed for frequent memory address randomization. 
This makes it very challenging for an attacker to ignore the key to inject executable code. 
This strategy disrupts the predictability of program execution and mitigates certain exploits such as speculative execution. 
We have not found any work that applies this strategy to \Wasm.

\wrule{ISA Randomization and Simulation}
This strategy involves using a key to cypher the original program binary into another encoded binary. 
Once encoded, the program can only be decoded at the target client, or it can be interpreted in the encoded form using a custom virtual machine implementation. 
This technique is strong against attacks involving code inspection. 
Kc \etal \cite{Kc03}, and Barrantes \etal \cite{barrantes2003randomized} proposed seminal works on instruction-set randomization 
to create a unique mapping between artificial CPU instructions and real ones.
On the same topic, Chew and Song \cite{Chew02mitigatingbuffer} target operating system randomization. They randomize the interface between the operating system and the user applications.
Courouss{\'e} \etal~\cite{courousse2016runtime} implement an assembly-like DSL to generate equivalent code at runtime in order to increase protection against side-channel attacks. Their technique generates a different program during execution using an interpreter for their DSL.
Code obfuscation \cite{wobfuscator} can be seen as a simplification of \emph{ISA randomization}. 
The main difference between encoding and obfuscating code is that the former requires the final target to know the encoding key while the latter executes as is in any client. 
Yet, both strategies aim to tackle program analysis from potential attackers. 
Moreover, this strategy faces a performance penalty, specially for \Wasm, due to the decoding process as shown in WASMixer evaluation \cite{wasmixer}.


Equivalence checking between program variants is a vital component for any program transformation task, ranging from checking compiler optimizations \cite{LeCompilers} to the artificial synthesis of programs discussed in this chapter. 
It proves that two pieces of code or programs are functionally equivalent \cite{churchill2019}. 
Cohen \cite{cohen1993operating} simplifies the checking process with the following property: two programs are equivalent if, given identical inputs, they produce identical outputs. 
We adopt this definition of \emph{functional equivalence} throughout this dissertation. 
In Software Diversification, equivalence checking seeks to preserve the original functionality of programs while varying observable behaviors. 
Two programs, for instance, can differ statically and still compute the same result. 
We outline two methods to check variant equivalence: by construction and automated equivalence checking.
In \autoref{tech}, we discuss the primary advantages and limitations in practice for both approaches within the scope of our contributions.

\wrule{Equivalence checking by construction:} The equivalence property is often guaranteed by construction. 
Cleemput \etal \cite{Cleemput2012} and Homescu \etal \cite{homescu2013profile}, for example, design their transformation strategies to generate semantically equivalent program variants. 
However, developer errors can occur in this process, necessitating further validation. 
The test suite of the original program can serve as a check for the variant. 
If the program variant passes the test suite \cite{harrand2020java}, it can be considered equivalent to the original. 
However, this technique is limited by the need for a preexisting test suite and does not give guarantees. 
Should the test suite not exist, an alternative technique is required for equivalence checking.
An alternative method for checking program equivalence involves the use of fuzzers \cite{zalewski2017american}.
Fuzzers randomly generate inputs that yield different observable behaviors. 
If two inputs produce a different output in the variant, the variant and the original program are not equivalent. 
The primary limitation for fuzzers is that the process is notably time-consuming and necessitates manual introduction of oracles.

\wrule{Automated equivalence checking:} In the absence of a test suite or a technique that inherently implements the equivalence property, the works mentioned earlier use theorem solvers (SMT solvers) \cite{SMT_solver} to prove equivalence. 
The central idea for SMT solvers is to convert the two code variants into mathematical formulas. 
The SMT solver then checks for counter-examples. When it finds a counter-example, there is an input for which the two mathematical formulas yield different outputs. 
The primary limitation of this technique is that not all algorithms can be translated into a mathematical formula, such as loops. 
Nevertheless, this technique is frequently used for checking no-branching-programs like basic block and peephole replacements \cite{SuperoptimizationScaling}.




\msubsection{Variants deployment}
Program variants, once generated and verified, may be utilized in two primary scenarios: Randomization or Multivariant Execution (MVE) \cite{jackson}. 
Additionally, these variants serve both defensive and offensive purposes \cite{offensive_div}.


\wrule{Randomization:}
In the context of our work, the term \emph{Randomization} denotes a program's ability to present different variants to different clients. 
In this setup, a program, chosen from a collection of variants (referred to as the program's variant pool), is assigned to a random client during each deployment. 
Jackson \etal \cite{jackson} define the variant pool in Randomization as herd immunity, as vulnerable binaries can only affect a segment of the client community. 
El-Khalil and colleagues \cite{ElKhalil2004} suggest employing a custom compiler to generate varying binaries from the compilation process. 
They adapt a version of GCC 4.1 to partition a conventional stack into several component parts, termed multistacks. 
Similarly, Singhal and colleagues, propose Cornucopia \cite{cornucopia}.
Cornucopia generates multiple variants of a program by using different compiler flag combinations.
Aga and colleagues \cite{aga2019smokestack}, contributing to this discussion, propose the generation of program variants through the randomization of its data layout in memory. 
This method allows each variant to operate on the same data in memory but at different memory offsets. 
Randomization can also be applied to virtual machines and operating systems. On this note, Kc \etal \cite{Kc03} establish a unique mapping between artificial CPU instructions and actual ones, enabling the assignment of various variants to specific target clients. 
In a similar vein, Xu \etal \cite{xu2020merr} recompile the Linux Kernel to minimize the exposure time of persistent memory objects, thereby increasing the frequency of address randomization.


\wrule{Multivariant Execution (MVE):}
Multiple program variants are composed into a single binary, known as a multivariant binary \cite{cox06}. 
Each multivariant binary is randomly deployed to a client.
Upon at the client, the multivariant binary executes its embedded program variants at runtime. 
These embedded variants can either execute in parallel to check for inconsistencies, or as a single program to randomize execution paths \cite{bhatkar03}. 
Bruschi and colleagues extend the concept of executing two variants in parallel, introducing non-overlapping and randomized memory layouts \cite{bruschi2007diversified}. 
At the same time, Salamat \etal modifies a standard library to generate 32-bit Intel variants. 
These variants have a stack that grows in the opposite direction, allowing for the detection of memory inconsistencies \cite{salamat2007stopping}. 
Davi and colleagues propose Isomeron, an approach for execution-path randomization \cite{davi2015isomeron}. 
Isomeron operates by simultaneously loading the original program and a variant. 
It then uses a coin flip to determine which copy of the program to execute next at the function call level. 
Previous works have highlighted the benefits of limiting execution to only two variants in a multivariant environment. 
Agosta and colleagues, as well as Crane and colleagues, used more than two generated programs in the multivariant composition, thereby randomizing software control flow at runtime \cite{agosta2015meet, crane2015thwarting}. 
Both strategies have proven effective in enhancing security by addressing known vulnerabilities, such as Just-In-Time Return-Oriented Programming (JIT-ROP) attacks \cite{jackson2011compiler} and power side-channel attacks \cite{amarilli2011can}. 
Lastly, only Voulimeneas \etal \cite{voulimeneas2021dmvx} have recently proposed a multivariant execution system that enhances security by parallelizing the execution of variants across different machines.

\wrule{Defensive Diversification:}
Lundquist and colleagues \cite{offensive_div} separate the usages of Software diversification into two categories: Defensive Software Diversification and Offensive Software Diversification.
Defensive Software Diversification is the traditional application of previously discussed techniques aimed at enhancing the security and reliability of software systems.
The core idea is to make it more difficult for attackers to predict the system's behavior and exploit program vulnerabilities. 
By doing so, even if an attacker manages to compromise one variant, the others may remain secure, thereby limiting the potential impact of the attack.
Defensive diversification is often complementary with other security measures, such as encryption and intrusion detection systems, to create a multi-layered defense strategy \cite{cryptography4020013}. 
%By making software more unpredictable and less uniform, defensive software diversification aims to preemptively thwart attacks, making it a complementary strategy to other, more reactive, security measures.

\wrule{Offensive Diversification:}
Offensive Software Diversification is a somewhat unconventional approach that uses the principles of software diversification, typically aimed at enhancing security.
Yet, in the offensive context, diversification techniques may be applied to malware or other malicious code to evade detection by security software \cite{8714698}.
For example, in the context of \wasm, the seminal work of Romano et al.  \cite{wobfuscator} proposed to intermix JavaScript and \wasm code to obfuscate JavaScript malware and make it more difficult to analyze for malware detectors.
The obfuscated version of the code is functionally equivalent to the original, being, in practice, Offensive Software Diversification.
Notice that, this method also measures the resilience and accuracy of security systems.


\msubsection{Open challenges}
\label{sota:openchallenges}
As outlined in \autoref{background:wasm:challenges}, our primary motivation for the contributions of this thesis is the open issues within the \Wasm ecosystem. 
We see potential in employing Software Diversification to address them. 
Based on our previous discussion, we highlight several open challenges in the realm of Software Diversification for \Wasm. 
First, \wasm\, being an emerging technology, is still in the process of implementing defensive measures \cite{Stevienart paper here}. 
The process of officially adopting a new defensive measure is inherently slow, making software diversification a potentially valuable preemptive strategy. 
Second, despite the abundance of related work on software diversity, its exploration in the context of \wasm\ remains limited. 
Third, both defensive and offensive software diversification have been largely unexplored. 
Notably, the works on malware detection discussed in \autoref{background:wasm:analysis} suggest that offensive diversification could be useful in measuring the resilience and accuracy of security systems for \Wasm.





% Besides, the process of diversifying a \Wasm program can be conceptualized as a three-stage procedure: parsing the program, transforming it, and finally re-encoding it back into \wasm. 
%Our review of the literature has revealed several studies that have employed parsing and encoding components for \wasm binaries across various domains as discussed in \autoref{background:wasm:analysis}. 
%This indicates that these works accept a \wasm binary as an input and output a unique \wasm binary. 
%When the transformation stage introduces randomized mutations to the original program, the aforementioned tools could potentially be built as diversifiers.
%However, we have not found any work that evaluates the impact of such tools as diversifiers.

